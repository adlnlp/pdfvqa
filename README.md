# PDFVQA


The github will be released right after the conference (ECML PKDD 2023).
The Appendix can be found in the Appendix.pdf 


# [PDF-VQA: A New Dataset for Real-World VQA on PDF Documents](https://arxiv.org/abs/2304.06447)

### <div align="center"> Yihao Ding, Siwen Luo, Hyunsuk Chung, Soyeon Caren Han </div>
### <div align="center"> Accepted by European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases <br> (ECML PKDD 2023) </div>



## PDFVQA Dataset
We introduce **PDFVQA**, a new Document-based Visual Question Answering dataset, to comprehensively examine the document understanding from various aspects, including document element recognition, document layout structural understanding as well as contextual understanding and key information extraction. Our PDF-VQA dataset extends the current scale of document understanding that limits on the single document page to the new scale that asks questions over the full document of multiple pages.

**The dataset contains 3 tasks:**
- **Task A**: document element recognition and spatial relationship understanding among document elements on the page-level. The answers should be predcited from a fixed answer space.
- **Task B**: structural understanding of document elements and answer extraction on the document page-level.
- **Task C**: document understanding on the full document-level of multiple consecutive pages.


- Detailed Dataset Statistics:

<img src="https://github.com/adlnlp/pdfvqa/blob/main/Figures/PDFVQA_DataStats.png" width="50%">
  

